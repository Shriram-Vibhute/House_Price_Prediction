{"cells":[{"metadata":{"_cell_guid":"6442c27f-a728-4503-966e-8b8b2f5c827f","_uuid":"9a07152bc9041c5d76ffd07cfafe1ac1805d26e1"},"cell_type":"markdown","source":""},{"metadata":{"_cell_guid":"1581d9fa-5027-4f68-bd62-5be5c9010f3d","_uuid":"99c61b7d13479ee7d900b1d4f964c49ffd70ebe3"},"cell_type":"markdown","source":""},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"The Boston Housing Dataset\n\nThe Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of [ Boston MA](http://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). The following describes the dataset columns:\n\n* CRIM - per capita crime rate by town\n* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n* INDUS - proportion of non-retail business acres per town.\n* CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n* NOX - nitric oxides concentration (parts per 10 million)\n* RM - average number of rooms per dwelling\n* AGE - proportion of owner-occupied units built prior to 1940\n* DIS - weighted distances to five Boston employment centres\n* RAD - index of accessibility to radial highways\n* TAX - full-value property-tax rate per \\$10,000\n* PTRATIO - pupil-teacher ratio by town\n* B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n* LSTAT - % lower status of the population\n* MEDV - Median value of owner-occupied homes in \\$1000's\n\n"},{"metadata":{"_cell_guid":"5f4d62ac-f9eb-457d-9d4d-9683a5a667cc","collapsed":true,"_uuid":"7d464c110dc186805b19e709b4443e66d407bdde","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d5b0617d-81ab-424f-a1ee-6674925f971e","_uuid":"b08753971c228268b0d2fba0a6978dfcfe9943f4","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.\nfrom pandas import read_csv\n#Lets load the dataset and sample some\ncolumn_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndata = read_csv('../input/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\nprint(data.head(5))","execution_count":98,"outputs":[]},{"metadata":{"_cell_guid":"fec83140-b3c5-4938-a031-6a34c2ebc45e","_uuid":"79d8bdae5e138554cb4626c784e0cfd43c40606d","trusted":true},"cell_type":"code","source":"# Dimension of the dataset\nprint(np.shape(data))","execution_count":99,"outputs":[]},{"metadata":{"_cell_guid":"3add881a-52cc-4645-9b2d-d88ecf7779e6","_uuid":"d863cc1813036123eff9879a9387bd980bf0d0ac","trusted":true},"cell_type":"code","source":"# Let's summarize the data to see the distribution of data\nprint(data.describe())","execution_count":100,"outputs":[]},{"metadata":{"_cell_guid":"b14f79c3-60ce-4d90-b0ec-7e69bd4bb186","_uuid":"94b4ddb51d694c35dbab357788b7e5c4517ecc39"},"cell_type":"markdown","source":"From get-go,  two data coulmns show interesting summeries. They are : ZN (proportion of residential land zoned for lots over 25,000 sq.ft.)  with 0 for 25th, 50th percentiles. Second, CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise) with 0 for 25th, 50th and 75th percentiles. These summeries are understandable as both variables are conditional + categorical variables. First assumption would be that these coulms may not be useful in regression task such as predicting MEDV (Median value of owner-occupied homes)."},{"metadata":{"_cell_guid":"5351963b-40ee-4d8b-9729-6a64d35758c3","_uuid":"8ce0ad3eb7208d0488057dc74dc06419fbfc210b"},"cell_type":"markdown","source":"Another interesing fact on the dataset is the max value of MEDV. From the original data description, it says: Variable #14 seems to be censored at 50.00 (corresponding to a median price of $50,000). Based on that, values above 50.00 may not help to predict MEDV. Let's plot the dataset and see interesting trends/stats."},{"metadata":{"_cell_guid":"23d0dfd5-b7a2-46e4-baed-10ca76a62dbc","_uuid":"50fd4b0697c8c6f9e30c6caa3f60c7d3a03d5a3d","trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in data.items():\n    sns.boxplot(y=k, data=data, ax=axs[index])\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","execution_count":101,"outputs":[]},{"metadata":{"_cell_guid":"e73e8b3a-daf7-49fa-8611-f3ce9a1b2c9c","_uuid":"4d4f0c23bb7761cfb67761df216a8b1bc2e20f75"},"cell_type":"markdown","source":"Columns like CRIM, ZN, RM, B seems to have outliers. Let's see the outliers percentage in every column."},{"metadata":{"_cell_guid":"3db29b4c-9c8c-4457-8064-91d6c3b5ed50","_uuid":"b80e456c7039e0d5c1c3f61e33cb8041ded81622","trusted":true},"cell_type":"code","source":"    for k, v in data.items():\n        q1 = v.quantile(0.25)\n        q3 = v.quantile(0.75)\n        irq = q3 - q1\n        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n        perc = np.shape(v_col)[0] * 100.0 / np.shape(data)[0]\n        print(\"Column %s outliers = %.2f%%\" % (k, perc))\n    \n    ","execution_count":102,"outputs":[]},{"metadata":{"_cell_guid":"6866af51-50df-4e4a-b268-e29420cb0a99","_uuid":"0251bc88b63788f481a2c05a0ab02acd640a3a8f"},"cell_type":"markdown","source":"Let's remove MEDV outliers (MEDV = 50.0) before plotting more distributions"},{"metadata":{"_cell_guid":"451b476c-6bb5-41b6-abe7-18721c4ea082","_uuid":"4a9d02c119dc56238c7071f1d1d2077708e6b649","trusted":true},"cell_type":"code","source":"data = data[~(data['MEDV'] >= 50.0)]\nprint(np.shape(data))","execution_count":103,"outputs":[]},{"metadata":{"_cell_guid":"b4f07967-903d-47ea-9bec-6153e8b18446","_uuid":"d75be26652e9370e490a535db7433f636767a1a8"},"cell_type":"markdown","source":"Let's see how these features plus MEDV distributions looks like"},{"metadata":{"_cell_guid":"3365b6f2-14dc-4ec3-9d6b-b5ea48b62971","_uuid":"ba686a43a8c707f42259c3254cb028ff97d0d104","trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in data.items():\n    sns.distplot(v, ax=axs[index])\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","execution_count":104,"outputs":[]},{"metadata":{"_cell_guid":"3a7c16fb-3c18-4d76-8ceb-602f43b90aef","_uuid":"a0a4b0a6a28538e9ad4df92da49856f599c25383"},"cell_type":"markdown","source":"The histogram also shows that columns CRIM, ZN, B has highly skewed distributions. Also MEDV looks to have a normal distribution (the predictions) and other colums seem to have norma or bimodel ditribution of data except CHAS (which is a discrete variable).\n\nNow let's plot the pairwise  correlation on data."},{"metadata":{"_cell_guid":"de1f6ba3-2aab-43ea-ab58-3f938b111ab5","_uuid":"a03fc465f35ebb73358874376569f2fe856c2763","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nsns.heatmap(data.corr().abs(),  annot=True)","execution_count":105,"outputs":[]},{"metadata":{"_cell_guid":"938c8cce-a377-4450-be3d-9d56e2b10f25","_uuid":"6740116517c45740b4c60b2626b6eb477051a52c"},"cell_type":"markdown","source":"From correlation matrix, we see TAX and RAD are highly correlated features. The columns LSTAT, INDUS, RM, TAX, NOX, PTRAIO has a correlation score above 0.5 with MEDV which is a good indication of using as predictors. Let's plot these columns against MEDV. "},{"metadata":{"_cell_guid":"8f8a04f5-b4b0-44dc-9034-6b937fe4a530","_uuid":"f03c64c06e33c098efef836457991d49056b7e2a","trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n# Let's scale the columns before plotting them against MEDV\nmin_max_scaler = preprocessing.MinMaxScaler()\ncolumn_sels = ['LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE']\nx = data.loc[:,column_sels]\ny = data['MEDV']\nx = pd.DataFrame(data=min_max_scaler.fit_transform(x), columns=column_sels)\nfig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor i, k in enumerate(column_sels):\n    sns.regplot(y=y, x=x[k], ax=axs[i])\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n","execution_count":106,"outputs":[]},{"metadata":{"_cell_guid":"54680e1a-addd-4a28-aac1-3065ecf941d2","_uuid":"321b79669416c5e71541539bb10e7c115e78e8ea"},"cell_type":"markdown","source":"So with these analsis, we may try predict MEDV with 'LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE' features. Let's try to remove the skewness of the data trough log transformation.\n\n"},{"metadata":{"trusted":true,"_uuid":"8355aaeb269fa9f8cf360d86a01754d43111731e"},"cell_type":"code","source":"y =  np.log1p(y)\nfor col in x.columns:\n    if np.abs(x[col].skew()) > 0.3:\n        x[col] = np.log1p(x[col])","execution_count":107,"outputs":[]},{"metadata":{"_uuid":"f10db586d1b7f15a5eeb441de373210790c41729"},"cell_type":"markdown","source":"Let's try Linear, Ridge Regression on dataset first."},{"metadata":{"_cell_guid":"4c67bb6b-b11e-4da7-906e-93c83ed85c39","_uuid":"4abf281773184265b8e52dad8d58aa72ba41b02a","trusted":true},"cell_type":"code","source":"from sklearn import datasets, linear_model\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\nl_regression = linear_model.LinearRegression()\nkf = KFold(n_splits=10)\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\nscores = cross_val_score(l_regression, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\nprint(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n\nscores_map = {}\nscores_map['LinearRegression'] = scores\nl_ridge = linear_model.Ridge()\nscores = cross_val_score(l_ridge, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\nscores_map['Ridge'] = scores\nprint(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n\n# Lets try polinomial regression with L2 with degree for the best fit\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n#for degree in range(2, 6):\n#    model = make_pipeline(PolynomialFeatures(degree=degree), linear_model.Ridge())\n#    scores = cross_val_score(model, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n#    print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\nmodel = make_pipeline(PolynomialFeatures(degree=3), linear_model.Ridge())\nscores = cross_val_score(model, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\nscores_map['PolyRidge'] = scores\nprint(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n","execution_count":108,"outputs":[]},{"metadata":{"_cell_guid":"241956e9-fa40-4524-b6fc-967d39a807ef","_uuid":"d427ce20b9f9e90a27b324b346fe61cabd8a6753"},"cell_type":"markdown","source":"The Liner Regression with and without L2 regularization does not make significant difference is MSE score. However polynomial regression with degree=3 has a better MSE. Let's try some non prametric regression techniques: SVR with kernal rbf, DecisionTreeRegressor, KNeighborsRegressor etc."},{"metadata":{"_kg_hide-input":false,"_cell_guid":"c8369e7b-0049-4648-ad23-f5f5d9530cd3","_kg_hide-output":false,"_uuid":"3fde7c8a019dcfdfaf60723bde8187923aea2108","trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\n\nsvr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n#grid_sv = GridSearchCV(svr_rbf, cv=kf, param_grid={\"C\": [1e0, 1e1, 1e2, 1e3], \"gamma\": np.logspace(-2, 2, 5)}, scoring='neg_mean_squared_error')\n#grid_sv.fit(x_scaled, y)\n#print(\"Best classifier :\", grid_sv.best_estimator_)\nscores = cross_val_score(svr_rbf, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\nscores_map['SVR'] = scores\nprint(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n","execution_count":109,"outputs":[]},{"metadata":{"_cell_guid":"650b9702-13cb-4364-b12f-f996fa013da4","_uuid":"2adbfea5a7d8b2262e71a6f2e9eed2187ce2576b","trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ndesc_tr = DecisionTreeRegressor(max_depth=5)\n#grid_sv = GridSearchCV(desc_tr, cv=kf, param_grid={\"max_depth\" : [1, 2, 3, 4, 5, 6, 7]}, scoring='neg_mean_squared_error')\n#grid_sv.fit(x_scaled, y)\n#print(\"Best classifier :\", grid_sv.best_estimator_)\nscores = cross_val_score(desc_tr, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\nscores_map['DecisionTreeRegressor'] = scores\nprint(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))","execution_count":110,"outputs":[]},{"metadata":{"_cell_guid":"c0d514db-3056-4a62-a702-979e746073df","_uuid":"ba681bc8dc9405517ed0cfaa2331c1b63b211f73","trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\n\nknn = KNeighborsRegressor(n_neighbors=7)\nscores = cross_val_score(knn, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\nscores_map['KNeighborsRegressor'] = scores\n#grid_sv = GridSearchCV(knn, cv=kf, param_grid={\"n_neighbors\" : [2, 3, 4, 5, 6, 7]}, scoring='neg_mean_squared_error')\n#grid_sv.fit(x_scaled, y)\n#print(\"Best classifier :\", grid_sv.best_estimator_)\nprint(\"KNN Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))","execution_count":111,"outputs":[]},{"metadata":{"_cell_guid":"34daccef-02e5-4b57-8f5b-6039ab0bd39f","_uuid":"d1ee1cbb5abca746bc21a0cccb537d5cf2f94f0a"},"cell_type":"markdown","source":"Compared to three models which are shosen through grid search, SVR performes better. Let's try an ensemble method finally."},{"metadata":{"_cell_guid":"ccee0ea9-9499-4b61-9bbb-f737e7fa2db9","_uuid":"2d6c452ab93d6413688439e100d7368d6019c33c","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\ngbr = GradientBoostingRegressor(alpha=0.9,learning_rate=0.05, max_depth=2, min_samples_leaf=5, min_samples_split=2, n_estimators=100, random_state=30)\n#param_grid={'n_estimators':[100, 200], 'learning_rate': [0.1,0.05,0.02], 'max_depth':[2, 4,6], 'min_samples_leaf':[3,5,9]}\n#grid_sv = GridSearchCV(gbr, cv=kf, param_grid=param_grid, scoring='neg_mean_squared_error')\n#grid_sv.fit(x_scaled, y)\n#print(\"Best classifier :\", grid_sv.best_estimator_)\nscores = cross_val_score(gbr, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\nscores_map['GradientBoostingRegressor'] = scores\nprint(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n","execution_count":112,"outputs":[]},{"metadata":{"_cell_guid":"cd261bb0-fcf4-4b9a-951f-0946b467dbc3","_uuid":"28bc1faf45827a4ec68fc72777e96fa84a065001"},"cell_type":"markdown","source":"Let's plot k-fold results to see which model has better distribution of results. Let's have a look at the MSE distribution of these models with k-fold=10"},{"metadata":{"_cell_guid":"8421b80d-d8bd-440f-be5f-0c75cc9a82e6","_uuid":"b65d37fc69ab9b8ced68f0ecc7fe6c11716f39f4","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nscores_map = pd.DataFrame(scores_map)\nsns.boxplot(data=scores_map)","execution_count":113,"outputs":[]},{"metadata":{"_cell_guid":"a16fe973-bbdd-4d4c-a14a-8ab357b9c918","_uuid":"7b6c29011f2e0791ed9239b760eec31fc2f1c37e"},"cell_type":"markdown","source":"The models SVR and GradientBoostingRegressor show better performance with -11.62 (+/- 5.91) and -12.39 (+/- 5.86).\n\nThis is my first kernel and thanks to https://www.kaggle.com/vikrishnan for the dataset and the well writtten kernel that provdies great pointers into this dataset."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}